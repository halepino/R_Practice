---
title: "Final Project Milestone 2"
author: "Holly Figueroa"
date: "5/22/2021"
output:
  pdf_document: default
  pdf: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning= FALSE, message=FALSE)
```

## Importing, Cleaning, Slicing, and Dicing the Data  

A central dataset to be used to ascertain relationships that may exist between variables and one's willingness to vaccinate is relatively clean to start.  Each variable was offered in a separate table. So each must file must be read, cleaned, and then combined. Each census table contains extra information in the first rows that will have to be skipped. Columns were selected for use and renamed.

```{r}
library(dplyr)
# WILLINGNESS CHANGES
# Open and clean willingness to vaccinate dataframe
orig_vaccine_df <- read.csv('final_project/vaccine_will.csv', skip=1)
head(orig_vaccine_df)
# rename columns and view
orig_vaccine_df <-orig_vaccine_df%>%
  rename(week = Week, 
         state = Area, 
         state_adult_pop = Total.Individual.Population.age.18., 
         willing_sample = Measure.Universe,
         total_willing = Number, 
         pc_MoE_willing = Percent.Margin.of.Error...., 
         pc_willing = Percent)
head(orig_vaccine_df)

# Select columns to keep and combine with others later.
vaccine_willing_percents <- orig_vaccine_df%>%
  select(state, pc_willing)
head(vaccine_willing_percents)

# EXPECTED INCOME LOSS CHANGES
# read expected loss of income due to Covid data file  
orig_exp_income_loss_df <- read.csv('final_project/exp_income_loss.csv', skip = 1)
head(orig_exp_income_loss_df)
colnames(orig_exp_income_loss_df)

# create percentages only data frames to combine later and rename columns
exp_income_loss_percents <- orig_exp_income_loss_df%>%
  select(2,7)%>%
  rename(state = Area, pc_exp_income_loss = Percent)
head(exp_income_loss_percents)

# INCOME LOST CHANGES
# read data on people with income lost due to Covid data file
orig_income_lost_df <-read.csv('final_project/income_lost.csv', skip = 1)
head(orig_income_lost_df)
colnames(orig_income_lost_df)

# create percentages only data frame to combine later and rename columns
income_lost_percents <-orig_exp_income_loss_df%>%
  select(2,7)%>%
  rename(state= Area, pc_income_lost = Percent)
head(income_lost_percents)

# EXPECTED EVICTION CHANGES
# read data file on people who anticipated eviction/foreclosure
orig_exp_eviction_df <-read.csv('final_project/eviction_likely.csv', skip = 1)
head(orig_exp_eviction_df)
colnames(orig_exp_eviction_df)

# create percentages only data frame to combine later and rename columns
exp_eviction_percents <- orig_exp_eviction_df%>%
  select(2,7)%>%
  rename(state = Area, pc_exp_eviction = Percent)
head(exp_eviction_percents)

# DELAYED MEDICAL CARE CHANGES
# read data file on people who delayed receiving medical care due to Covid
orig_delayed_med_df <- read.csv('final_project/delayed_med.csv', skip = 1)
head(orig_delayed_med_df)
colnames(orig_delayed_med_df)
```

All of variables chosen to be combined are expressed as percentages of respondents that answered yes to particular questions. These were combined into a single data frame of census variables only, called "my_data". After some changes were made, other issues were discovered. Each data frame from the census survey includes rows of data on metro cities as opposed to the state. To address this, metro cities was separated out from the state data and set aside for potential use.This was done using filter functions and slicing functions. I do not have election data at this level of measurement, however, so any analysis of city metro populations would not involve election variables. Data included from the census also has a first row including the United States as a whole. This was also taken out and set aside for potential reference. Combining data was relatively easy as all rows were organized by state name. 


```{r}

#create percentages only data frame to combine later and rename columns
delayed_med_percents <- orig_delayed_med_df%>%
  select(2,7)%>%
  rename(state = Area, pc_delayed_med = Percent)
head(delayed_med_percents)

# Combine data frames into one, check, and tidy
my_data <-cbind(vaccine_willing_percents,
                exp_income_loss_percents,
                income_lost_percents,
                exp_eviction_percents,
                delayed_med_percents)
colnames(my_data)

# Take out duplicate state columns
my_data<-my_data%>%
  select(-3, -5, -7, -9 )
head(my_data)

# Separate out city metro data into separate file
library(stringr)
metro_data <- my_data%>%
  filter(str_detect(state, "Metro"))%>%
  rename(location = state)
head(metro_data)

# Slice out metro data from my_data
nrow(my_data)
my_data <- slice(my_data,c(1:52))

# Separate out United States level of observation
us_census_data <- my_data%>%
  filter(state == "United States")
head(us_census_data)

# Slice out United States level of observations so only data on 51 states remains
my_data <- slice(my_data, c(2:52))
nrow(my_data)
  

```

The election data retrieved from Kaggle.com is given at the county level. To get state percentages of vote by any candidate the total vote for each state much be tallied. Once grouped by state, total votes per state can be gained. After that votes for Donald Trump can be filtered and totaled. Dividing votes for Donald Trump by the total votes gains a percentage of state presidential votes for Donald Trump. By doing this, the data can share measurement scale at both the state level, and as percentages of values. 

```{r}

# ELECTION DATA CHANGES

# PRESIDENTIAL DATA
election2020_state_and_county <- read.csv('final_project/president_county_candidate.csv')
head(election2020_state_and_county)

# Get total pres votes by state
election2020<-election2020_state_and_county%>%
  group_by(state)%>%
  summarise_at(vars(total_votes), list(total_votes = sum))

# get republican pres votes by state
rep_votes <- election2020_state_and_county%>%
  filter(candidate == "Donald Trump")%>%
  group_by(state)%>%
  summarise_at(vars(total_votes), list(trump_votes = sum))%>%
  select(trump_votes)

head(rep_votes)

# Combine columns: state,total presidential votes, and total presidential votes 
election2020 <- cbind(election2020,rep_votes)

# Create percentage republican presidential votes column
election2020$trump_percentage <- (election2020$trump_votes / election2020$total_votes) * 100

#Rename column to specify presidential total votes
election2020 <- election2020%>%
  rename(total_pres_votes = total_votes)
head(election2020)

# Explore table
summary(election2020)
```

After further digging, I also found issues with election data for non-presidential elections. As re-elections vary due to term limits and other reasons, it was not possible to gather complete republican election percentages at other levels. I have yet decided how best to address this, so for now, data regarding republican party dominance by state will not have the nuance of including other offices of power, such as house seats and senate seats gained during the November election of 2020.

## Final Data Set  

With all the variables combined the complete data set contains the following variables for analysis:

|Variable Name |Variable Meaning|
|-----|-----|
|state| state|
|pc_willing|percentage of individuals planning or willing to vaccinate once able|
|pc_exp_income_loss|percentage of individuals that anticipated a loss of income in the next 4 weeks|
|pc_income_lost|percentage of households where someone had a loss in employment income in the last 7 days|
|pc_ex_eviction|percentage of individuals that expected eviction or home foreclosure in the next two months|
|trump_percentage|percentage of votes that were won by Donald Trump out of all presidential votes cast|

```{r}
# COMBINE ALL VARIABLES AT THE STATE LEVEL INTO ONE DATA FRAME

nrow(my_data)
nrow(election2020)
combined_data <- merge(x = my_data, y = election2020)
head(combined_data)

combined_data <- combined_data%>%
  select(-trump_votes,-total_pres_votes)
# Head final table
head(combined_data)

#Rename columns to avoid issues calling up the data by shared starting text
combined_data <- combined_data%>%
  rename(wiling_pc = pc_willing, 
                        exp_income_loss_pc = pc_exp_income_loss,
                        income_lost_pc = pc_income_lost,
                        exp_eviction_pc = pc_exp_eviction,
                        delayed_med_pc = pc_delayed_med)

head(combined_data)

```


## Questions for future steps  

While my initial data sets were very large, by any measure, my approach has left me with 51 rows of data. As such, any analysis is at a great disadvantage. Furthermore, any single outlier in state data will bring my small sample down again, if removed. If I can find a way to combine all the data on the county level I will have ample data. To do so would require some careful cleaning to separate counties by name and match them. I am confident the variables from the survey and the election to not share the same amount of specified counties. So my questions largely rest there, learning how to correctly separate the strings to match and merge. 

## What information is not self-evident?
While I have information on all the sample sizes used to obtain data, they are not included in this final data set. That may pose issues. Margins of error are given in the census data, and may be more accurate than those I would obtain on the data I see. While I initially intended to include variables of race and ethnicity, but I have chosen to not include them at this time. 
    
## What are different ways you could look at this data?
I think it would be to my benefit to try and gain values for my variables at the county level to expand my sample for analysis. Including variables for state populations in the final data frame could lend some insights. Seeing relationships between variables, such as income loss and expected eviction could lend insight into the severity of problems state populations reported.
   
## How could you summarize your data to answer key questions?
Maximum, minimum, median, and mean values would all lend insights into the distributions and shape of the frequencies of each variable. Any regressions will benefit from summary output as well. Multiple regression analysis would be the optimal way to summarize the current data set. Sharing findings from the summary function to add and compare models as parameters are added would be appropriate. Offering the R squared and adjusted R squared statistics would also be appropriate. 

## What types of plots and tables will help you to illustrate the findings to your questions?
Distributions of variables, visually will be informative. Residual plots for the predictors would be useful. Correlation plots of each variable, or at least any that show significance would be illustrative. I plan to include my table of variables to better explain what each measures.  

## Do you plan on incorporating any machine learning techniques to answer your research questions? Explain.
If I gain confidence in using machine learning techniques, I may conclude they would be useful to employ. However, given the small number of variables, that may not be necessary. 
    
## Questions for future steps
Is my data set of no use at this size? Does it even conform to the assignment constraints even though they were obtained from much larger set? I may find this out on my own shortly, but I would like to know. How will changing my scope to the county level create issues for me down the line. If the data obtained regarding elections and survey variables are not from the same exact source of individuals, how do i have to adjust my analyses. Are there other measures for political influence on willingness to vaccinate that might serve well?